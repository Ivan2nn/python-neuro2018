{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch # import the PyTorch package\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision # import trochvision package\n",
    "from torchvision import transforms # get torchvision's transforms subpackage\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite transform that first converts images to tensors and then normalize the images\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converts images into Tensors\n",
    "    transforms.Normalize([0.1307], [0.3081])\n",
    "])\n",
    "\n",
    "# apply the transforms at the time of dataset loading\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)\n",
    "\n",
    "batch_size = 64\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decently complex network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a much more complex network (although still would be considered very simple from the field's standard) that uses operations like **convolution** and **drop outs**. (Covering these opartions is beyond the scope of this course but you can find tons of references on them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.296712\n",
      "Epoch 0 Loss: 2.031971\n",
      "Epoch 0 Loss: 0.999476\n",
      "Epoch 0 Loss: 0.840974\n",
      "Epoch 0 Loss: 0.603185\n",
      "Epoch 0 Loss: 0.603220\n",
      "Epoch 0 Loss: 0.928216\n",
      "Epoch 0 Loss: 0.530968\n",
      "Epoch 0 Loss: 0.419187\n",
      "Epoch 0 Loss: 0.497543\n",
      "Epoch 1 Loss: 0.482797\n",
      "Epoch 1 Loss: 0.325747\n",
      "Epoch 1 Loss: 0.406913\n",
      "Epoch 1 Loss: 0.346881\n",
      "Epoch 1 Loss: 0.379779\n",
      "Epoch 1 Loss: 0.326885\n",
      "Epoch 1 Loss: 0.525812\n",
      "Epoch 1 Loss: 0.437641\n",
      "Epoch 1 Loss: 0.328717\n",
      "Epoch 1 Loss: 0.279459\n",
      "Epoch 2 Loss: 0.596667\n",
      "Epoch 2 Loss: 0.304374\n",
      "Epoch 2 Loss: 0.506606\n",
      "Epoch 2 Loss: 0.239122\n",
      "Epoch 2 Loss: 0.458214\n",
      "Epoch 2 Loss: 0.248173\n",
      "Epoch 2 Loss: 0.433848\n",
      "Epoch 2 Loss: 0.568583\n",
      "Epoch 2 Loss: 0.127778\n",
      "Epoch 2 Loss: 0.282033\n",
      "Epoch 3 Loss: 0.138029\n",
      "Epoch 3 Loss: 0.204866\n",
      "Epoch 3 Loss: 0.310613\n",
      "Epoch 3 Loss: 0.217657\n",
      "Epoch 3 Loss: 0.309295\n",
      "Epoch 3 Loss: 0.263177\n",
      "Epoch 3 Loss: 0.151063\n",
      "Epoch 3 Loss: 0.261064\n",
      "Epoch 3 Loss: 0.375558\n",
      "Epoch 3 Loss: 0.206957\n",
      "Epoch 4 Loss: 0.131428\n",
      "Epoch 4 Loss: 0.338209\n",
      "Epoch 4 Loss: 0.099059\n",
      "Epoch 4 Loss: 0.252143\n",
      "Epoch 4 Loss: 0.136737\n",
      "Epoch 4 Loss: 0.207311\n",
      "Epoch 4 Loss: 0.104399\n",
      "Epoch 4 Loss: 0.114328\n",
      "Epoch 4 Loss: 0.272818\n",
      "Epoch 4 Loss: 0.161939\n",
      "Epoch 5 Loss: 0.179763\n",
      "Epoch 5 Loss: 0.222183\n",
      "Epoch 5 Loss: 0.135199\n",
      "Epoch 5 Loss: 0.102951\n",
      "Epoch 5 Loss: 0.130809\n",
      "Epoch 5 Loss: 0.176573\n",
      "Epoch 5 Loss: 0.232821\n",
      "Epoch 5 Loss: 0.214158\n",
      "Epoch 5 Loss: 0.157793\n",
      "Epoch 5 Loss: 0.271545\n",
      "Epoch 6 Loss: 0.152644\n",
      "Epoch 6 Loss: 0.336277\n",
      "Epoch 6 Loss: 0.277511\n",
      "Epoch 6 Loss: 0.179712\n",
      "Epoch 6 Loss: 0.106457\n",
      "Epoch 6 Loss: 0.187728\n",
      "Epoch 6 Loss: 0.099157\n",
      "Epoch 6 Loss: 0.112470\n",
      "Epoch 6 Loss: 0.127777\n",
      "Epoch 6 Loss: 0.272684\n",
      "Epoch 7 Loss: 0.141400\n",
      "Epoch 7 Loss: 0.107086\n",
      "Epoch 7 Loss: 0.192429\n",
      "Epoch 7 Loss: 0.306380\n",
      "Epoch 7 Loss: 0.315489\n",
      "Epoch 7 Loss: 0.290542\n",
      "Epoch 7 Loss: 0.331392\n",
      "Epoch 7 Loss: 0.196798\n",
      "Epoch 7 Loss: 0.660874\n",
      "Epoch 7 Loss: 0.322141\n",
      "Epoch 8 Loss: 0.145155\n",
      "Epoch 8 Loss: 0.145687\n",
      "Epoch 8 Loss: 0.066845\n",
      "Epoch 8 Loss: 0.052976\n",
      "Epoch 8 Loss: 0.106356\n",
      "Epoch 8 Loss: 0.170725\n",
      "Epoch 8 Loss: 0.132054\n",
      "Epoch 8 Loss: 0.184913\n",
      "Epoch 8 Loss: 0.207911\n",
      "Epoch 8 Loss: 0.296110\n",
      "Epoch 9 Loss: 0.198539\n",
      "Epoch 9 Loss: 0.254490\n",
      "Epoch 9 Loss: 0.159918\n",
      "Epoch 9 Loss: 0.187607\n",
      "Epoch 9 Loss: 0.147847\n",
      "Epoch 9 Loss: 0.072582\n",
      "Epoch 9 Loss: 0.061769\n",
      "Epoch 9 Loss: 0.110410\n",
      "Epoch 9 Loss: 0.139008\n",
      "Epoch 9 Loss: 0.159871\n",
      "Training completed in 160.31 seconds\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "start = time.time()\n",
    "for epoch_idx in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))\n",
    "            \n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 58941/60000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this network can perform much better than our earlier networks, but it takes significantly longer to train!\n",
    "\n",
    "For your refernce the best network performance on MNIST to date is 99.79% on the test set! You can find the latest classification scores on MNIST and many other popular benchmark datasets [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up your training with a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a significant speed up by placing the network and data on GPUs and letting computation take place there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING** The following code will only work if you are on a machine with a properly configured GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.to('cuda') # place the network on GPU!\n",
    "\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "start = time.time()\n",
    "for epoch_idx in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # send each batch to GPU so it can be processed by the network that's also on the GPU\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))\n",
    "            \n",
    "duration = time.time() - start\n",
    "print('Training completed in {:.2f} seconds'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also test the network on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        # place batch onto GPU\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "        \n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
