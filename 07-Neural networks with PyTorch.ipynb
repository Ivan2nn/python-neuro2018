{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # import the PyTorch package\n",
    "import torchvision # import trochvision package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors - enhanced NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.Tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why PyTorch Tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensors can be run on **GPUs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load a **training set** that we are going to use to train our network and a separate **test set** that we'll use to evaluate the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use convenience methods in `torchvision.datasets` to download various popular machine learning benchmark images. Here we are going to download [**MNIST**](http://yann.lecun.com/exdb/mnist/) which is a collection of handwritten digits along with labels (i.e. what digit was drawn).\n",
    "\n",
    "The MNIST dataset consists of a total of 70,000 images, of which 60,000 are desginated as the **training set** and 10,000 are designated as the **test set**. This standardized separation allows everyone around the world to evaluate and compare their model's performances with each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST('./data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns Torchvision's special **dataset** object that can be used to represent **supervised datasets** consisting of both inputs (i.e. images) and targets (i.e. digit labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_set[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Digit: 5')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)\n",
    "plt.title('Digit: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the test set in an identical fashion, passing in `train=False` into `MNIST`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.MNIST('./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_set[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Digit: 1')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image)\n",
    "plt.title('Digit: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even before you start feeding in your images into a neural network, it is very common to perform some data transformations - modifying images in some fixed manner that makes it easier to work with them.\n",
    "\n",
    "One of the most common image transformation is **normalization**, where you first compute mean and standard deviation across all images (typically in the training set). You then subtract the mean from each image and also divide each image by the standard deviation. If you did this, and recomputed the mean and standard deviation across all images, you will find that they now have **mean of 0** and **standard deviation of 1**, and thus they are said to be **normalized**.\n",
    "\n",
    "Normalization helps ensure input image intensities stay within some expected range, allowing the network to not have to worry about large variations in image values that is otherwise visually uninteresting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when you load images from Torchvision, they are provided as Pillow package's Image object. Pillow is one of Python's popular image processing package, and there images are represented by a dedicated Image object with a lot of methods implementing common image processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in PyTorch, networks only understands PyTorch Tensors, and thus we must convert the images from Pillow Image into PyTorch Tensor before we can pass the image into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve these two *transformations* by making use of Torchvision's transformation operations. You combine multiple transformation operations together and pass it at the time of dataset loading. This returns a dataset that **applies these transformations** automatically on all images! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a transformation that will:\n",
    "1. convert images into PyTorch tensors\n",
    "2. normalize the images against the mean of 0.1307 and standard deviation of 0.3081."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms # get torchvision's transforms subpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite transform that first converts images to tensors and then normalize the images\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converts images into Tensors\n",
    "    transforms.Normalize([0.1307], [0.3081])\n",
    "])\n",
    "\n",
    "# apply the transforms at the time of dataset loading\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
    "                                          transform=image_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now any image you access through the dataset has the transformation already applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, you define a new neural network by defining a **new class that inherits from nn.Module** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(5, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        z = F.relu(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better, let's take a quick review of classes and learn the new concept of **object inheritance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past session, we have taken a look at defining a **class** to represent a grouping of data and functions, where each **instance of a class** or **object** can be thought of as representing a concrete unit that has **properties** and **behavior** (or **methods**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # assign name\n",
    "        \n",
    "    def title(self):\n",
    "        return \"an ordinary person.\"\n",
    "    \n",
    "    def greeting(self):\n",
    "        print('Hello! My name is {}.'.format(self.name))\n",
    "        print(\"I'm {}\".format(self.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = Person('Edgar')\n",
    "john = Person('John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is Edgar.\n",
      "I'm an ordinary person.\n"
     ]
    }
   ],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is John.\n",
      "I'm an ordinary person.\n"
     ]
    }
   ],
   "source": [
    "john.greeting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that both `edgar` and `john` are objects of type (class) `Person`. They both have properties called `name` that is unique to each, and have the common behaviors (method) called `greeting` that prints out a greeting message introducing themselves. Note that `greeting` method calls another method `title` in creating the intro statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key of Object-Oriented Programming (OOP) is to group certain data (e.g. `name`) with behavior (e.g. `greeting`, `title`) that when put together can be used to represent a conceptual grouping that may correspond to some real world *objects*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialization via inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that you want to define a new **class** of object called `Scientist` that has everything that a `Person`  has (e.g.`name`, `greeting`, and `title`), but has extra property called `topic` that specified their research topic, and has a new behavior (i.e. *method*) called `research` that finds a significant result at p-value < 0.05. \n",
    "\n",
    "Without worry much about code duplication, you could implement it as such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Scientist:\n",
    "    def __init__(self, name, topic):\n",
    "        self.name = name\n",
    "        self.topic = topic\n",
    "        \n",
    "    def title(self):\n",
    "        return \"an ordinary person.\"\n",
    "    \n",
    "    def greeting(self):\n",
    "        print('Hello! My name is {}.'.format(self.name))\n",
    "        print(\"I'm {}\".format(self.title()))\n",
    " \n",
    "    def research(self, silent=False):\n",
    "        print('Performing a research on the topic {}...'.format(self.topic))\n",
    "        pvalue = random.random() # randomly pick a value between [0, 1)\n",
    "        \n",
    "        if pvalue < 0.05:\n",
    "            if not silent:\n",
    "                print('Results statistically significant with p-value={:0.3f}!! Publish!!'.format(pvalue))\n",
    "            return True\n",
    "        else:\n",
    "            if not silent:\n",
    "                print('Results was not significant with p-value={:0.3f}... Continue working...'.format(pvalue))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = Scientist(name='Edgar', topic='computational neuroscience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is Edgar.\n",
      "I'm an ordinary person.\n"
     ]
    }
   ],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing a research on the topic computational neuroscience...\n",
      "Results was not significant with p-value=0.793... Continue working...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgar.research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that there is a lot of repeated code between a `Person` and a `Scientist`. Both has property called `name` and a method called `greeting`. After all, a Scientist **is a** Person, right? When one class can be thought of as a **specialization** of another class, you can save alot of typing and code duplication by using **class inheritance**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Scientist(Person):  # Scientist inherits from Person\n",
    "    def __init__(self, name, topic):\n",
    "        super().__init__(name)\n",
    "        self.topic = topic\n",
    "\n",
    "    def research(self, silent=False):\n",
    "        print('Performing a research on the topic {}...'.format(self.topic))\n",
    "        pvalue = random.random() # randomly pick a value between [0, 1)\n",
    "        \n",
    "        if pvalue < 0.05:\n",
    "            if not silent:\n",
    "                print('Results statistically significant with p-value={:0.3f}!! Publish!!'.format(pvalue))\n",
    "            return True\n",
    "        else:\n",
    "            if not silent:\n",
    "                print('Results was not significant with p-value={:0.3f}... Continue working...'.format(pvalue))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "moku = Scientist(name='Moku', topic='physics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is Edgar.\n",
      "I'm an ordinary person.\n"
     ]
    }
   ],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing a research on the topic computational neuroscience...\n",
      "Results statistically significant with p-value=0.048!! Publish!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgar.research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `Scientist` class no longer implements the `greeting` and `eat` methods, yet you can still call them on the instance of `Scientist`. This is because these methods were **inherited** from `Person` class.\n",
    "\n",
    "Furthermore, we call something funny inside the `__init__` method: `super().__init__(name)`. As you may be able to guess, this calls the initializer of `Person` or the **super class**, passing in the value it expects (e.g. `name` of the person). This allows any complex configuration that `Person` might have done in its `__init__` to be reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can **override** super class's implementation of a method to give a new, specialized behavior to an existing method! Here, let's **override** the implementation of the method `title`, in effect cusotmizing the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Scientist(Person):  # Scientist inherits from Person\n",
    "    def __init__(self, name, topic):\n",
    "        super().__init__(name)\n",
    "        self.topic = topic\n",
    "        \n",
    "    # overriding title method\n",
    "    def title(self):\n",
    "        return \"a scentist in {}\".format(self.topic)\n",
    "\n",
    "    def research(self, silent=False):\n",
    "        print('Performing a research on the topic {}...'.format(self.topic))\n",
    "        pvalue = random.random() # randomly pick a value between [0, 1)\n",
    "        \n",
    "        if pvalue < 0.05:\n",
    "            if not silent:\n",
    "                print('Results statistically significant with p-value={:0.3f}!! Publish!!'.format(pvalue))\n",
    "            return True\n",
    "        else:\n",
    "            if not silent:\n",
    "                print('Results was not significant with p-value={:0.3f}... Continue working...'.format(pvalue))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = Scientist(name='Edgar', topic='computational neuroscience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is Edgar.\n",
      "I'm a scentist in computational neuroscience\n"
     ]
    }
   ],
   "source": [
    "edgar.greeting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we were able to modify the behaivor of an already existing method `greeting` by overriding the behavior of the another method `title`. This pattern in which you can **customize** behavior of an already existing mechanism by overriding another method happens quite commonly. In fact, we will soon encounter them in implementing our neural network in PyTorch!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks inherit from *nn.Module*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now armed with knowlege of class inheritance, let's take another look at a typical definition of a neural network in PyTorch. In PyTorch, any network **is a** `nn.Module`. In other words, you define a new class that inherits from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(5, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        z = F.relu(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that `MyNetwork` *inherits from* `nn.Module`. This gives our class `MyNetwork` with a lot of properties and methods that is already part of `nn.Module`, and this is precisely what allows you to define a new neural network with minimal work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network in PyTorch (which is a class), typically consists of one or more **layers** that you create and hold on to inside the `__init__` method. Here, we are defining a single `nn.Linear` layer which corresponds to a *fully-connected* linear layer connecting from 5 input nodes into 10 output nodes. We **instantiate** this layer and assign it to the object's property named `fc` (standing for **f**ully **c**onnected layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the `__init__`, we have not computed anything. We simply instantiated a layer and assigned it to a property for *later use*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real use of a PyTorch module comes in when you **instantiate** the class - that is, you create an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action just created a new **instance** of the network, with it's own network weights that can be trained!\n",
    "\n",
    "A key feature of a module is the fact that you can use it like a function - it accepts an input and returns an output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0695,  0.4837,  0.0000,  0.0000,  0.2656,  0.0000,  0.0172,\n",
       "          0.0000,  0.3369,  0.0000]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 5) # a simple vector of 5 elements - or 5 input values\n",
    "\n",
    "y = net(x) # you use a module instance like a function!\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secret behind this is the `forward` method we defined in the `MyNetwork` class:\n",
    "\n",
    "```python\n",
    " def forward(self, x):\n",
    "    y = self.fc(x)\n",
    "    z = F.relu(y)\n",
    "    return z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, we accepted a parameter `x`, and we used the fully-connected linear layer `self.fc` as a function with `x` as the input!\n",
    "\n",
    "It turns out that `nn.Linear` is yet another **subclass** of `nn.Module` and thus can take input, return output. Our particular `self.fc` was configured to take in input of size 5 and output vector of size 10.\n",
    "\n",
    "The `F.relu` is then an element-wise operation that clips any value less than 0 to 0, while keeping positive values as is. `forward` function finally returns the output of `relu` and that becomes the output of the whole network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, `MyNetwork` implemented a **single full-connected layer neural network with ReLU output non-linearity** - one of the simplest networks you can construct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building network to classify images into digits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen how to build a network by defining a new class that inherits from `nn.Module`, let's try to implement a network that takes in a $28 \\times 28$ pixels gray scale image of a digit and classifies them into 1 of 10 digits!\n",
    "\n",
    "The input will be one or more images of size $28 \\times 28$, and we are going to set **the output to be a vector of size 10** where each position indicates a *log probability* that the image belongs to the specific digit.\n",
    "\n",
    "We'll start with a simplest possible implementation where we **flatten out** the input image into a vector of size 28 * 28 = 784. This will be **fully connected neural network with no output nonlinearity** linking 784 input nodes into 10 output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flattens an image of form N x 1 x 28 x 28 -> N x 784\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1) # make sure that probabilities add up to one, and then take log\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the network and run an image through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_set[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x118ac4cf8>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9762, -2.1093, -2.4735, -2.6918, -3.4270, -3.2984, -1.9790,\n",
       "         -2.4868, -1.4873, -2.6417]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a log of class probabilities, so we can exponentiate this to get the actual probability over classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1386,  0.1213,  0.0843,  0.0678,  0.0325,  0.0369,  0.1382,\n",
       "          0.0832,  0.2260,  0.0712]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(net(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perhaps take the index with the largest probability as the network's best guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.exp(net(image))\n",
    "torch.argmax(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that our network is not quite performing well. That's expected because our network is **randomly initialized**! In order to get a reasonable performance, we need to **train** the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to **train our network** by minimizing a **loss function** - a function that evaluates how *off* we are from the true target of the world. Chosing a good loss function can influence how well your network performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of **N-way classification** problem where the output is a vector of size *N*, it's quite common to treat the output as the log probability of N classes, and optimize the network by miniminzg **negative log likelihood**. \n",
    "\n",
    "This is conceptually similar to adjusting the network weights (parameters) so that the correct class would have the higest probability among choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training a neural network, you would follow a procedure called **gradient descent** to adjust the values of the network weights such that the loss function becomes smaller.\n",
    "\n",
    "One of the biggest strenghts of frameworks like PyTorch lies in the fact **it can compute gradient of the loss with respect to all parameters in the network automatically** for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On gradient descent and back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient of the loss with respect to the weights, neural network packages like PyTorch (and pretty much any other similar packages) make use of technique called **back propagation**. Covering the details of gradient descent and how back propagation exactly works are outside the scope of this course. Interested readers are strongly encouraged to refer to wonderful online resources such as [Neural Network and Deep Learning](http://neuralnetworksanddeeplearning.com/) online text book by Michael Nielson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a full-fledged optimization problem, you will typically evaluate the loss on **all training dataset** and try to **optimize the joint loss**. However, this requires computing the loss on all images everytime you make a small modification to the parameters, and for complex neural networks, this computational cost can be extremely prohibitive.\n",
    "\n",
    "Hence, you would typically **estimate** the joint loss by evaluating the loss on a randomly selected subsets of the input-target pairs, or on **a minibatch** (or simply a batch) of data.\n",
    "\n",
    "Performing gradient descent on randomly sampled subsets of the training set is known as (minibatch) **stochastic gradient descent** (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to perform minibatch SGD, we need a way to construct a random minibatch from the training datasets. Fortunately, this is easy to achive using PyTorch's `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size) # by default shuffle is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` is an iterable, that returns batches of input target pairs of specified *batch size*. We can take a look at what it will return by calling `next` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([64, 1, 28, 28])\n",
      "Labels: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, t in training_loader:\n",
    "    print('Images:', x.shape)\n",
    "    print('Labels:', t.shape)\n",
    "    break # quit after one iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for both the images (inputs) and the labels (targets), the **first dimension is the batch dimension**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through the minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to get a minibatch, let's start putting together a training framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # MISSING! Perform back propagation to compute gradient and perform gradient descent step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code successuflly steps through 60,000 training images in batch of 64, evaluate the network on the inputs, and computes the loss between the network prediction and the targets. \n",
    "\n",
    "However, we are still missing the step of computing the gradient of loss with respect to the parameters of the network, and performing gradient descent to actually adjust and therefore **train** the network parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve this by using:\n",
    "\n",
    "1. `backward` method call on the finall loss to trigger backpropagation computation, and \n",
    "2. An **optimizer** to adjust the values of the parameters based on the gradient\n",
    "\n",
    "Putting this all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # reset the gradient before the next gradient step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # perform back propagation to compute gradients with respect to parameters!\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a gradient descent step on the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final missing piece is some sort of **monitoring** by which we can observe that the network is actually training (that is, the loss decreases over training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.534343\n",
      "Loss: 0.816979\n",
      "Loss: 0.542766\n",
      "Loss: 0.726783\n",
      "Loss: 0.331407\n",
      "Loss: 0.422346\n",
      "Loss: 0.351978\n",
      "Loss: 0.291532\n",
      "Loss: 0.455459\n",
      "Loss: 0.251536\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # reset the gradient before the next gradient step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # perform back propagation to compute gradients with respect to parameters!\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a gradient descent step on the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # report the loss every 100 batches\n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Loss: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed our network appears to train over time as shown by the fact loss decreses over iterations. You can also see that loss eventually stop decreasing when the training reaches the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the speed of the training by changing the value of the **learning rate**. Loosely speaking, learning rate controls the size of the step you step in each gradient descent step. \n",
    "\n",
    "Larger learning rate could lead to faster training but it can also easily stray youself away from an optimal solution. Achieving a good network training depends a lot on a good choice of the value of the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained network, it's time to evaluate it's performance on the test set. Training on one set and testing on a distinct set that was **not** used during the training is called **cross-validation**, and can be a good way to evaluate how well your network will **generalize** beyond the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3928, Accuracy: 53447/60000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that our network actually performs ~84-90% correct on the digit classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the earlier example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_set[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.exp(net(image))\n",
    "torch.argmax(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a more complex network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we were already getting well above chance performance on digit classification with an extremely simple **single fully-connected layer network**. Now, let's try improving the result by training slight more complex network - **three layer fully-connected network with ReLU nonlinearity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flattens an image of form N x 1 x 28 x 28 -> N x 784\n",
    "        x = F.relu(self.fc1(x)) # first fully connected layer followed by ReLU\n",
    "        x = F.relu(self.fc2(x)) # second fully connected layer followed by ReLU\n",
    "        x = self.fc3(x) # third fully connected layer *without* output ReLU\n",
    "        x = F.log_softmax(x, dim=1) # make sure that probabilities add up to one, and then take log\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not go ahead and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.260793\n",
      "Loss: 2.121841\n",
      "Loss: 1.914793\n",
      "Loss: 1.571293\n",
      "Loss: 1.103959\n",
      "Loss: 0.857417\n",
      "Loss: 0.685158\n",
      "Loss: 0.515869\n",
      "Loss: 0.545789\n",
      "Loss: 0.567869\n"
     ]
    }
   ],
   "source": [
    "net = ComplexNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(training_loader):\n",
    "    # reset the gradient before the next gradient step\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # evaluate the network output\n",
    "    output = net(data)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = F.nll_loss(output, target)\n",
    "    \n",
    "    # perform back propagation to compute gradients with respect to parameters!\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a gradient descent step on the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # report the loss every 100 batches\n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Loss: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice two things:\n",
    "1. It trains much slower! and\n",
    "2. The loss is still decreasing at the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is slower because the nework is more complex and it requires much more computations. To deal with the fact that it is still decreasing, we should not limit ourselves to a single pass through our training set, but rather go through numerous pass through it. Each complete pass through the training set is called **an epoch**, so here we want to perform **multiple epoch** training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.320441\n",
      "Epoch 0 Loss: 2.113369\n",
      "Epoch 0 Loss: 1.858312\n",
      "Epoch 0 Loss: 1.407332\n",
      "Epoch 0 Loss: 1.143449\n",
      "Epoch 0 Loss: 0.923603\n",
      "Epoch 0 Loss: 0.662661\n",
      "Epoch 0 Loss: 0.707626\n",
      "Epoch 0 Loss: 0.493586\n",
      "Epoch 0 Loss: 0.470204\n",
      "Epoch 1 Loss: 0.587930\n",
      "Epoch 1 Loss: 0.495217\n",
      "Epoch 1 Loss: 0.349844\n",
      "Epoch 1 Loss: 0.402586\n",
      "Epoch 1 Loss: 0.542727\n",
      "Epoch 1 Loss: 0.418397\n",
      "Epoch 1 Loss: 0.283267\n",
      "Epoch 1 Loss: 0.326697\n",
      "Epoch 1 Loss: 0.344775\n",
      "Epoch 1 Loss: 0.342889\n",
      "Epoch 2 Loss: 0.352867\n",
      "Epoch 2 Loss: 0.266470\n",
      "Epoch 2 Loss: 0.329063\n",
      "Epoch 2 Loss: 0.264309\n",
      "Epoch 2 Loss: 0.202597\n",
      "Epoch 2 Loss: 0.302214\n",
      "Epoch 2 Loss: 0.264094\n",
      "Epoch 2 Loss: 0.276803\n",
      "Epoch 2 Loss: 0.283225\n",
      "Epoch 2 Loss: 0.295514\n",
      "Epoch 3 Loss: 0.146708\n",
      "Epoch 3 Loss: 0.312303\n",
      "Epoch 3 Loss: 0.234566\n",
      "Epoch 3 Loss: 0.300298\n",
      "Epoch 3 Loss: 0.180963\n",
      "Epoch 3 Loss: 0.271392\n",
      "Epoch 3 Loss: 0.459493\n",
      "Epoch 3 Loss: 0.214845\n",
      "Epoch 3 Loss: 0.262167\n",
      "Epoch 3 Loss: 0.267412\n",
      "Epoch 4 Loss: 0.267329\n",
      "Epoch 4 Loss: 0.235646\n",
      "Epoch 4 Loss: 0.263253\n",
      "Epoch 4 Loss: 0.262632\n",
      "Epoch 4 Loss: 0.236005\n",
      "Epoch 4 Loss: 0.172437\n",
      "Epoch 4 Loss: 0.213288\n",
      "Epoch 4 Loss: 0.408838\n",
      "Epoch 4 Loss: 0.374095\n",
      "Epoch 4 Loss: 0.323541\n"
     ]
    }
   ],
   "source": [
    "net = ComplexNetwork()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
    "\n",
    "for epoch_idx in range(5):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2466, Accuracy: 55753/60000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much more complex network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a much more complex network (although still would be considered very simple from the field's standard) that uses operations like **convolution** and **drop outs**. (Covering these opartions is beyond the scope of this course but you can find tons of references on them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.342847\n",
      "Epoch 0 Loss: 2.149011\n",
      "Epoch 0 Loss: 1.522278\n",
      "Epoch 0 Loss: 0.739555\n",
      "Epoch 0 Loss: 0.912529\n",
      "Epoch 0 Loss: 0.745104\n",
      "Epoch 0 Loss: 0.576485\n",
      "Epoch 0 Loss: 0.645698\n",
      "Epoch 0 Loss: 0.293432\n",
      "Epoch 0 Loss: 0.514020\n",
      "Epoch 1 Loss: 0.371585\n",
      "Epoch 1 Loss: 0.333495\n",
      "Epoch 1 Loss: 0.330696\n",
      "Epoch 1 Loss: 0.449758\n",
      "Epoch 1 Loss: 0.321991\n",
      "Epoch 1 Loss: 0.455858\n",
      "Epoch 1 Loss: 0.357260\n",
      "Epoch 1 Loss: 0.316326\n",
      "Epoch 1 Loss: 0.297177\n",
      "Epoch 1 Loss: 0.442172\n",
      "Epoch 2 Loss: 0.394849\n",
      "Epoch 2 Loss: 0.206427\n",
      "Epoch 2 Loss: 0.305661\n",
      "Epoch 2 Loss: 0.340804\n",
      "Epoch 2 Loss: 0.740139\n",
      "Epoch 2 Loss: 0.276811\n",
      "Epoch 2 Loss: 0.292988\n",
      "Epoch 2 Loss: 0.425271\n",
      "Epoch 2 Loss: 0.168602\n",
      "Epoch 2 Loss: 0.263998\n",
      "Epoch 3 Loss: 0.205135\n",
      "Epoch 3 Loss: 0.201637\n",
      "Epoch 3 Loss: 0.378599\n",
      "Epoch 3 Loss: 0.265469\n",
      "Epoch 3 Loss: 0.316506\n",
      "Epoch 3 Loss: 0.177646\n",
      "Epoch 3 Loss: 0.129650\n",
      "Epoch 3 Loss: 0.210935\n",
      "Epoch 3 Loss: 0.244079\n",
      "Epoch 3 Loss: 0.192322\n",
      "Epoch 4 Loss: 0.366677\n",
      "Epoch 4 Loss: 0.123705\n",
      "Epoch 4 Loss: 0.192520\n",
      "Epoch 4 Loss: 0.508474\n",
      "Epoch 4 Loss: 0.232916\n",
      "Epoch 4 Loss: 0.234764\n",
      "Epoch 4 Loss: 0.116563\n",
      "Epoch 4 Loss: 0.292540\n",
      "Epoch 4 Loss: 0.281311\n",
      "Epoch 4 Loss: 0.167966\n",
      "Epoch 5 Loss: 0.347682\n",
      "Epoch 5 Loss: 0.127306\n",
      "Epoch 5 Loss: 0.157396\n",
      "Epoch 5 Loss: 0.359966\n",
      "Epoch 5 Loss: 0.529964\n",
      "Epoch 5 Loss: 0.233545\n",
      "Epoch 5 Loss: 0.186869\n",
      "Epoch 5 Loss: 0.098398\n",
      "Epoch 5 Loss: 0.263673\n",
      "Epoch 5 Loss: 0.125684\n",
      "Epoch 6 Loss: 0.162054\n",
      "Epoch 6 Loss: 0.242695\n",
      "Epoch 6 Loss: 0.231620\n",
      "Epoch 6 Loss: 0.237484\n",
      "Epoch 6 Loss: 0.182548\n",
      "Epoch 6 Loss: 0.224056\n",
      "Epoch 6 Loss: 0.204901\n",
      "Epoch 6 Loss: 0.213687\n",
      "Epoch 6 Loss: 0.227319\n",
      "Epoch 6 Loss: 0.279910\n",
      "Epoch 7 Loss: 0.167275\n",
      "Epoch 7 Loss: 0.062055\n",
      "Epoch 7 Loss: 0.220356\n",
      "Epoch 7 Loss: 0.170566\n",
      "Epoch 7 Loss: 0.123787\n",
      "Epoch 7 Loss: 0.486534\n",
      "Epoch 7 Loss: 0.182062\n",
      "Epoch 7 Loss: 0.202704\n",
      "Epoch 7 Loss: 0.503137\n",
      "Epoch 7 Loss: 0.124340\n",
      "Epoch 8 Loss: 0.230396\n",
      "Epoch 8 Loss: 0.135000\n",
      "Epoch 8 Loss: 0.157971\n",
      "Epoch 8 Loss: 0.099914\n",
      "Epoch 8 Loss: 0.238549\n",
      "Epoch 8 Loss: 0.128967\n",
      "Epoch 8 Loss: 0.102402\n",
      "Epoch 8 Loss: 0.069860\n",
      "Epoch 8 Loss: 0.051456\n",
      "Epoch 8 Loss: 0.120742\n",
      "Epoch 9 Loss: 0.241285\n",
      "Epoch 9 Loss: 0.104983\n",
      "Epoch 9 Loss: 0.169138\n",
      "Epoch 9 Loss: 0.305346\n",
      "Epoch 9 Loss: 0.132243\n",
      "Epoch 9 Loss: 0.326395\n",
      "Epoch 9 Loss: 0.209387\n",
      "Epoch 9 Loss: 0.241897\n",
      "Epoch 9 Loss: 0.205517\n",
      "Epoch 9 Loss: 0.211154\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.train() # puts the network into the training mode\n",
    "\n",
    "# create and initialize an optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch_idx in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(training_loader):\n",
    "        # reset the gradient before the next gradient step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate the network output\n",
    "        output = net(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # perform back propagation to compute gradients with respect to parameters!\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step on the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # report the loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} Loss: {:.6f}'.format(epoch_idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0586, Accuracy: 58962/60000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval() # put network into evaluation model\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# prevents unnecessary gradient computation during test - can lead to time and memory saving\n",
    "with torch.no_grad(): \n",
    "    for data, target in test_loader:\n",
    "        output = net(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() \n",
    "        \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        # count number of times where max probability matches the label index\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# divide the test loss by number of samples in the test set\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
